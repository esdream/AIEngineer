# 机器学习复习知识点

### 牛顿法

1. 一元函数牛顿法迭代公式（忽略2次泰勒展开2次以上的项）：

   $x_{t+1} = x_t - \frac {f'(x_t)} {f''(x_t)}$

2. 多元函数牛顿法迭代公式（忽略2次泰勒展开2次以上的项）：

   $x_{k+1} = x_k - H^{-1}_k g_k$

3. 牛顿法的完整流程：

   1. 给定初始值$x_0$和精度阈值$\epsilon$，设置k  = 0
   2. 计算梯度$g_k$和矩阵$H_k$
   3. 如果$||g_k|| < \epsilon$，即在此点处梯度的值接近于0，达到极值点处，停止迭代
   4. 计算搜索方向$d_k = -H^{-1}_k g_k$

   5. 计算新的迭代点$x_{k+1} = x_k + \gamma d_k$

   6. 令 k = k + 1，返回步骤2

      其中$\gamma$是一个人工设定的接近于0的常数，保证$x_{k+1}$在$x_k$的邻域内，从而可以忽略泰勒展开的高次项。如果目标函数是**二次函数**，Hessian矩阵是一个常数矩阵，对于任意给定的初始点，牛顿法只需要一次迭代就可以收敛到极值点。

4. 在实现中，一般不直接求Hessian矩阵的逆矩阵，而是求解下面的线性方程：

   $H_kd = -g_k$

   其解d称为**牛顿方向**。迭代终止的判定依据是梯度值充分接近于0，或者达到最大迭代次数。

5. 牛顿法比梯度下降法收敛速度更快，但每次迭代需要计算Hessian矩阵，求解一个线性方程组，运算量大。如果Hessian矩阵不可逆，该方法失效。

### 凸优化

1. 凸优化的两个限制条件：

   1. 对于目标函数，我们限定是凸函数
   2. 对于优化变量的可行域，我们限定它是凸集

2. 对于n维空间中点的集合C，如果对集合中的任意两点x和y，以及实数 $0 \le \theta\le 1 $，都有

   $\theta x + (1 - \theta) y \in C$

   则该集合称为凸集。如果把这个集合画出来，其边界是凸的，没有凹进去的地方。

### 拉格朗日对偶

1. SVM中的目标函数是最大化间隔分类器与样本的间隔gap。假设间隔分类器的权值为$w$，则目标函数为

   $max \frac {1} {||w||} s.t. , y_i(w^Tx_i + b) \ge 1, i = 1, ... , n$

2. 由于求$\frac {1} {||w||}$的最大值相当于求$\frac {1} {2} ||w||^2$的最小值，所以上述目标函数等价于

   $ min \frac {1} {2} ||w||^2, s.t. , y_i(w^Tx_i + b) \ge 1, i = 1, ... , n $

3. 原始问题



### SVM

1. 高斯核函数

   $f_i  = similarity(x. l^{(i)}) = exp( - \frac {||x - l ^ {i}||^2} {2 \sigma ^ 2})$

2. Softmax函数

   将一个含任意实数的k维向量z压缩到另一个k维实向量$\sigma(z)$中，使得每一个元素的范围都在(0, 1)之间，并且所有元素的和为1。

   $\sigma (z)_j = \frac {e^{z_j}} {\Sigma^k_{K=1}e^{z_k}}, for  j = 1, ..., K$

   其中$z = w_k^Tx$。 

### 高斯分布

1. 一维高斯分布

   $f(x) = \frac {1} {\sigma \sqrt{2 \pi}} exp ( - \frac {(x - \mu) ^ 2} {2 \sigma ^ 2})$

2. 多维高斯分布

   $f_x(x_1, ..., x_k) = \frac {1} {\sqrt{(2 \pi)^k|\Sigma|}} exp ( - \frac {1} {2} (x - \mu)^T \Sigma^{-1}(x - \mu))$

