# 机器学习复习知识点

### 泰勒展开式

用g(x)去逼近f(x)，前面n项为展开项，最后一项（n+1项）为拉格朗日余项。当$x$无限接近$x_0$时，余项是$(x - x_0)^n$的高阶无穷小。其中$\epsilon \in (x_0, x)$，即最后一项可以在$(x_0, x)$之间任意位置展开。

$$
f(x) = g(x) = g(x_0) + \frac {f^1(x_0)} {1!}(x - x_0) + \frac {f^2(x_0)} {2!}(x - x_0) ^ 2 + ... + \frac {f^n(x_0)} {n!}(x - x_0) ^ n + \frac {f^{n+1}(\epsilon)} {(n+1)!}(x - x_0) ^ {n+1}
$$


### 牛顿法

1. 一元函数牛顿法迭代公式（忽略2次泰勒展开2次以上的项）：

   $x_{t+1} = x_t - \frac {f'(x_t)} {f''(x_t)}$

2. 多元函数牛顿法迭代公式（忽略2次泰勒展开2次以上的项）：

   $x_{k+1} = x_k - H^{-1}_k g_k$

3. 牛顿法的完整流程：

   1. 给定初始值$x_0$和精度阈值$\epsilon$，设置k  = 0
   2. 计算梯度$g_k$和Hessian矩阵$H_k$
   3. 如果$||g_k|| < \epsilon$，即在此点处梯度的值接近于0，达到极值点处，停止迭代
   4. 计算搜索方向$d_k = -H^{-1}_k g_k$

   5. 计算新的迭代点$x_{k+1} = x_k + \gamma d_k$

   6. 令 k = k + 1，返回步骤2

      其中$\gamma$是一个人工设定的接近于0的常数，保证$x_{k+1}$在$x_k$的邻域内，从而可以忽略泰勒展开的高次项。如果目标函数是**二次函数**，Hessian矩阵是一个常数矩阵，对于任意给定的初始点，牛顿法只需要一次迭代就可以收敛到极值点。

4. 在实现中，一般不直接求Hessian矩阵的逆矩阵，而是求解下面的线性方程：

   $H_kd = -g_k$

   其解d称为**牛顿方向**。迭代终止的判定依据是梯度值充分接近于0，或者达到最大迭代次数。

5. 牛顿法比梯度下降法收敛速度更快，但每次迭代需要计算Hessian矩阵，求解一个线性方程组，运算量大。如果Hessian矩阵不可逆，该方法失效。

### 凸优化

1. 凸优化的两个限制条件：

   1. 对于目标函数，我们限定是凸函数
   2. 对于优化变量的可行域，我们限定它是凸集

2. 对于n维空间中点的集合C，如果对集合中的任意两点x和y，以及实数 $0 \le \theta\le 1 $，都有

   $\theta x + (1 - \theta) y \in C$

   则该集合称为凸集。如果把这个集合画出来，其边界是凸的，没有凹进去的地方。

### 拉格朗日对偶

1. SVM中的目标函数是最大化间隔分类器与样本的间隔gap。假设间隔分类器的权值为$w$，则目标函数为

   $max \frac {1} {||w||} s.t. , y_i(w^Tx_i + b) \ge 1, i = 1, ... , n$

2. 由于求$\frac {1} {||w||}$的最大值相当于求$\frac {1} {2} ||w||^2$的最小值，所以上述目标函数等价于

   $ min \frac {1} {2} ||w||^2, s.t. , y_i(w^Tx_i + b) \ge 1, i = 1, ... , n $

3. 原始问题



### SVM

1. 高斯核函数

   $f_i  = similarity(x. l^{(i)}) = exp( - \frac {||x - l ^ {i}||^2} {2 \sigma ^ 2})$

2. Softmax函数

   将一个含任意实数的k维向量z压缩到另一个k维实向量$\sigma(z)$中，使得每一个元素的范围都在(0, 1)之间，并且所有元素的和为1。

   $\sigma (z)_j = \frac {e^{z_j}} {\Sigma^k_{K=1}e^{z_k}}, for  j = 1, ..., K$

   其中$z = w_k^Tx$。 

### 高斯分布

1. 一维高斯分布

   $f(x) = \frac {1} {\sigma \sqrt{2 \pi}} exp ( - \frac {(x - \mu) ^ 2} {2 \sigma ^ 2})$

2. 多维高斯分布

   $f_x(x_1, ..., x_k) = \frac {1} {\sqrt{(2 \pi)^k|\Sigma|}} exp ( - \frac {1} {2} (x - \mu)^T \Sigma^{-1}(x - \mu))$

### 回归模型评估

1. 平均绝对值误差（MAE）

   $MAE= \frac {1} {n} \Sigma^n_{i=1}|y_i -  \hat y_i|$

2. 均方误差（MSE）

   $MSE = \frac {1} {n} \Sigma^n_{i=1} (y_i - \hat y_i)^2$

3. R平方值（注意与相关系数中的$R^2$区分）

   $R^2(y, \hat y) = 1 - \frac {\Sigma^{n_{samples} - 1}_{i=0} (y_i - \hat y_i)^2} {\Sigma^{n_{samples} - 1}_{i=0} (y_i - \overline y_i)^2}$ 

   $R^2(y, \hat y)$越接近1，模型越好；越接近0，模型越差。

### 决策树

1. 剪枝思路：使用测试集检测某一节点分裂前后，对测试集拟合结果的差别（《机器学习》上的方法）。

   + 预剪枝：先假定节点不划分，计算测试集拟合结果，再计算划分后测试集拟合结果，比较决定是否剪枝。
   + 后剪枝：先生成完整决策树，在比较去除某一分支节点后，拟合结果的变化决定是否剪枝。

   一般情况下，**后剪枝**决策树的欠拟合风险很小，泛化性能往往优于先预剪枝。

   在《统计学习方法》中，有更详细的对剪枝的数学描述（P65起）。

   #### ID3/C4.5剪枝

   决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）实现。设树T的叶节点个数为|T|，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类样本点有$N_{tk}$个，k = 1, 2, ..., K。$H_t(T)$为叶结点t上的经验熵。$\alpha \ge 0$为参数。则决策树学习的损失函数可以定义为：

   $$C_{\alpha}(T) = \Sigma^{|T|}_{t=1}N_tH_t(T) + \alpha|T|$$

   其中经验熵为

   $H_t(T) = -\Sigma_k \frac {N_{tk}} {N_T} log \frac {N_{tk}} {N_T}$

   令

   $C(T) = \Sigma^{|T|}_{t=1}N_tH_t(T) = -\Sigma^{|T|}_{t=1}\Sigma_k \frac {N_{tk}} {N_T} log \frac {N_{tk}} {N_T}$

   则有

   $C_{\alpha}(T) = C(T) + \alpha|T|$

   其中$C(T)$表示模型对训练数据的**预测误差（如这里的经验熵（条件熵））**，$|T|$表示模型复杂度，参数$\alpha \ge 0$控制两者之间的影响，这里的$\alpha$是**人为确定**的，较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促进选择较复杂的模型。

   #### CART剪枝

   + 核心思想：**第一步依据训练集，先生成初始决策树$T_0$。然后做t次裁剪（t为叶子结点个数），每次只剪掉一个叶子结点（例如叶子结点共3个ABC，先剪掉A，保留B，C；再剪掉B，保留A，C），计算$\alpha = \frac {C(t) - C(T_t)} {|T_t| -1 }$, 取最小的那个$\alpha$对应的结点，剪掉，生成第一个子树$T_1$。重复以上步骤，直到最后只剩下根节点，作为最后一个子树。得到子树序列$T_0, T_1, ..., T_n$，最后使用验证集对所有子树做一个prediction，取误差最小的子树和对应的$\alpha$**。

   + 具体步骤：

     输入：CART算法生成的决策树$T_0$。

     输出：最优决策树$T_{\alpha}$。

     (1) 设k = 0，$T = T_0$.

     (2) 设$\alpha = + \infty$.

     (3) 自下而上地对各内部结点t计算$C(T_t)$，$|T|$以及

     $g(t) = \frac {C(t) - C(T_t)} {|T_t| -1 }$

     $\alpha = min(\alpha, g(t))$

     这里$g(t)$表示**误差增益（剪枝前后损失函数相等时，求解得到的$\alpha$）**，$T(t)$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。

     (4) 自上而下地访问内部结点t，如果有$g(t) = \alpha$，进行剪枝，并对叶结点t以多数表决法决定其类，得到树T。

     (5) 设$k = k + 1, \alpha_k = \alpha, T_k = T$。

     (6) 如果T不是由根结点单独构成的树，则回到步骤(4)。

     (7) 采用交叉验证法在子树序列$T_0, T_1, ..., T_n$中选取最优子树$T_{\alpha}$。 

     与ID3/C4.5剪枝不同的是，CART剪枝的$\alpha$不是人为确定的，而是遍历各个决策树方案得到的。

2. 缺失值处理：为每个样本x赋予一个权重$w_x$，如果X在划分属性a上取值未知，则将x同时划入所有子结点，且计算信息增益时，使用样本权值计算，这就是让一个样本以**不同概率**划分到不同的子结点中去，详细见《机器学习》P87。

3. Bagging

   采用自助采样法（bootstrap sampling），有放回地采样，不被采样到的样本比例如下公式，最终得到一个约为初始训练集**63.2%**的train dataset。不被采样的样本作为测试集。

   $lim_{m \to \infty} (1 - \frac {1} {m}) ^ m \to \frac 1 e \approx 0.368$

4. 随机森林（Random Forest）

   在Bagging基础上发展出来。传统决策树在选择划分属性时是在当前结点的属性结合中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中**随机选择一个包含k个属性的子集**，然后再从中选择一个最优属性划分。推荐值$k  = log_2d$。

### 分布距离度量

#### 相对熵（KL散度，Kullback-Leibler divergence）

设p(x)和q(x)是X取值的两个概率分布，则p对q的相对熵为
$$
D(p||q) = \Sigma^n_{i=1}p(x)log\frac {p(x)} {q(x)}
$$

KL散度是两个概率分布P和Q的非对称性的度量，其度量的是使用基于Q的编码来编码来自于P的样本平均所需的**额外位元数**。典型形况下，P表示数据的真实分布，Q表示数据的理论分布/模型分布。

##### 相对熵的性质

相对熵有两个主要的性质：

+ KL散度不具有对称性，即$D(p||q) \ne D(q||p)$
+ 相对熵的值为非负值，即$D(p||q) \ge 0$

##### 相对熵的应用

相对熵可以衡量两个**随机分布**之间的距离，当两个随机分布相同时，它们的相对熵为0，当两个随机分布的差别增大时，它们的相对熵也会增大，因此相对熵（KL散度）可以用于比较文本的相似度，先统计出词的频率，然后计算KL散度。

#### JS散度（Jensen-Shannon）

JS散度度量了两个概率分布的相似度，是基于KL散度的变体，解决了KL散度**非对称的问题**，一般地，JS散度是对称的，其取值是0到1之间。
$$
JS(P_1 || P_2) = \frac {1} {2} KL(P_1 || \frac {P_1 + P_2 } {2}) +  \frac {1} {2} KL(P_2 || \frac {P_1 + P_2 } {2})
$$
KL散度和JS散度都存在一个问题：如果两个分布P, Q**完全没有重叠**时，则KL散度没有意义，JS散度值是一个常数，这意味着此时梯度为0，梯度就消失了。因此在GAN的改进WGAN中使用了更有效的Wassertein距离。

#### Wasserstein散度

$$
W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{x,y \sim \gamma} [||x - y||]
$$

Wasserstein相比于JS散度和KL散度的优点：两个分布分别处于**完全没有覆盖的低维流形中**时，Wasserstein距离仍然能**保证有意义**且**能够连续地表示**两个分布的距离。

### 极大似然估计

#### 重要前提

+ 训练样本的分布能代表样本的真实分布。

+ 每个样本都是独立同分布的随机变量（IID条件）。
+ 有充足的训练样本。

#### 原理

极大似然估计提供了一种给定观察数据来评估模型参数的方法，即“模型已定，参数未知”。通过若干次实验，观察结果，利用实验结果得到某个参数值使样本出现的概率最大，称为极大似然估计。

#### 步骤

求最大似然估计量$\hat{\theta}$的一般步骤：

1. 写出似然函数。
2. 对似然函数取对数，并整理（取对数的原因是为了避免数值下溢，指计算机浮点数计算的效果小于可以表示的最小数，即小于$2^{-127}$）。
3. 求导数。
4. 解似然方程。

详细的推导过程见[极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849)。

#### 特点

1. 比其他估计方法更加简单。
2. 收敛性：无偏或者渐进无偏，当样本数目增加时，收敛性质会更好。
3. 如果假设的类条件概率模型正确，通常能获得较好的效果；但如果假设模型出现偏差，将导致非常差的估计效果。



