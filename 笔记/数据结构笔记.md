---
title: 数据结构笔记
tags: data structure
grammar_cjkRuby: true
---
## 绪论
---
### 计算模型
---
1. 程序与计算的关系：
	DSA = data structure + algorithm  = program
	(data strcuture + algorithm) * efficiency = computation
2. Turing Machine(图灵机模型)：
	+ Tape(条带)
		依次均匀地划分为单元格，每个单元格标注有一个字符，默认为`#`
	+ Alphabet(字母表)
		字符的种类是有限的，由`Alphabet`(字母表)决定
	+ Head(读写头)
		总是对准某一单元格，并可读取和改写其中的字符，每经过一个节拍，Head可以移动至左侧或右侧的邻格
	+ State(状态)
		图灵机总是处于有限种装填中的某一种，每经过一个节拍，可按照规则转向另一种状态
	+ Transition Function: (q, c; d, L/R, p)
		转换函数: (参数)
		若当前状态为`q`且当前字符为`c`，则将当前字符**改写**为`d`；然后**转向**左侧/右侧的邻格，**转入**`p`状态；一旦转入特定状态`h`，则**停机**
3. RAM: Ramdom Access Machine(随机存取机模型)
	+ 寄存器顺序编号，总数没有限制
		R[0], R[1], R[2], ...
	+ 每一基本操作仅需常数时间
		例如： R[i] <- c     R[i] <- R[j]    IF R[i] = 0 GOTO l    STOP
4. TM和RAM模型都是用于一般计算工具的简化与抽象，是我们可以独立于具体的计算平台，对算法的效率做出可信的比较和评判。在这些模型中，算法的运行时间被转化为算法需要执行的基本操作次数。
### DSA复杂度表示
---
1. 大O记号
	T(n) = O(f(n)) 用大O记号表示长远情况下的T(n)（即操作步数）。
	这有一个充分必要条件，存在c > 0，当n >> 2后，有T(n) < c * f(n)
	即n远大于2时，对T(n)放大并做简化，获得长远情况下DSA的上界，即算法在最坏情况下的运行效率。
	与T(n)相比，f(n)更加简洁，但依然反应前者的增长趋势。其中：
	+ 常系数可以忽略：O(f(n)) = O(c * f(n))
	+ 低次项可以忽略：O(n^a^ + n^b^) = O(n^a^), a > b > 0
		绝大部分情况下，讨论DSA的运行效率考虑的是最坏情况，因此使用最多的评价符号是大O。
2. 大Ω记号
	T(n) = Ω(f(n))：存在c > 0，当n >> 2后，有T(n) > c * f(n)
	大Ω确定的是长远情况下DSA的下界，即算法在最好情况下的运行效率。
3. Θ记号
	T(n) = Θ(f(n))：存在c1 > c2 > 0，当n >> 2后，有c1 * f(n) > T(n) > c2 * f(n)，其中c1, c2都是正的常数，
	Θ表示长远情况下DSA运行效率的确界。
4. 高效解：
	+ 常数复杂度（O(1)）
		**最高效**算法的复杂度。
		不含转向（循环、调用、递归等），只由分支、判断构成的代码体，其必然是顺序执行，则必是O(1)。
	+ 对数多项式复杂度（O(log^c^n)）
		这类算法非常**有效**，复杂度无限接近于常数
5. 有效解
 	+ 多项式复杂度（O(n^c^)）
		线性复杂度：所有O(n)类函数。
		凡多项式复杂度的算法，都是**可解**的。
6. 难解
	+ 指数复杂度(O(a^n^) , a为常数)
		这类算法的计算成本增长极快，通常认为不可忍受。
		从O(n^c^)到O(2^n^)是从有效算法到无效算法的**分水岭**。
### 算法分析
---
1. 两个主要任务 = 正确性(不变性×单调性)+复杂度
+ 单调性：问题的有效规模会随着算法的推进不断递减。
+ 不变性：与最终的正确性相呼应，当问题的有效规模缩减至0时，不变性应等价与正确性。例如起泡排序中，经过k趟扫描交换后，最大的前k个元素必然就位。
	存在以下基本事实：`C++`等高级语言的基本指令，均**等效于**常数条RAM的基本指令，在渐进意义下，二者答题相当。
	+ 分支转向 = goto    // 算法的灵魂，为了结构化而被隐藏
	+ 迭代循环 = for(), while(), ...   // 本质上就是 if + goto
	+ 调用 + 递归    // 本质上也是goto
2. 复杂度分析的主要方法
	+  迭代：级数求和
	+  递归：递归跟踪+递推方程
	+  猜测+验证
3. 级数
	+ 幂方级数：比幂次高出一阶
		Ta(n) = 1^a^ + 2^a^ + 3^a^ + ... + n^a^ = O(n^a+1^), 其中a>0
	+ 几何级数(a>1):与末项同阶
		Ta(n) = a^0^ + a^1^ + ... + a^n^ = (a^n+1^ - 1)/(a - 1) = O(a^n^)
	+ 收敛级数
		1/1/2 + 1/2/3 + ... + 1/(n-1)/n = 1 - 1/n = O(1)
	+ 可能未必收敛，然而长度有限
		h(n) = 1 + 1/2 + 1/3 + ... 1/n = Θ(logn)	// 调和级数
		log1 + log2 + log3 + ... + logn = log(n!) = Θ(nlogn)	// 对数级数
4. 封底计算
	通过估算计算所用时间和n的规模，大致得到该算法运行所需要的时间。
### 迭代和递归
---
1. 通常考量空间复杂度时，考虑的是除了输入本身所占的空间之外，我们所需要的附加的用于计算所必须的空间总量。
2. 减而治之
	为求解一个大的问题，可以将其划分为两个子问题：其一是平凡情况下的问题（例如长度为1，或只有1个元素的子问题），另一个是规模递减的子问题
	分别求解，得到原问题的解
3. 递归分析的技巧
	+ 递归跟踪
	检查每个递归实例，累计所需时间，其总和即算法执行时间。其特点是直观形象，但仅适用于简明的递归模式。
	+ 递推方程
		例如：T(n) = T(n-1) + O(1)		// 递推
			  T(0) = O(1)		// 递推基
				求解： T(n) - n  = T(n-1) - (n-1) = ... = T(2) - 2 = T(1) - 1 = T(0)
				所以：T(n) = O(1) + n  = O(n)
4. 减而治之
	为求解一个大的问题，将其划分为若干（通常是两个）子问题，规模大体相当于分别求解子问题，由子问题的解，得到原问题的解。
### 动态规划
---
1. 子序列
	由序列中若干字符，按原相对次序构成。
	最长公共子序列就是两个学列公共子序列中的最长者。
2. 递归：设计出可行且正确的解。
3. 动态规划：消除重复计算，提高效率，将递归转化为迭代。
	动态规划的核心，是 **状态的定义** 与 **状态转移方程** 。详细的原理介绍见 [什么是动态规划？动态规划的意义是什么？](https://www.zhihu.com/question/23995189)。

## 向量
---
### 接口与实现
---
1. Abstract Data Type vs. Data Strcuture
	抽象数据类型(Abstract Data Type) = 数据类型 + 定义在该模型上的一组操作
	数据结构(Data Strcuture) = 基于某种特定语言，实现ADT的一整套算法
	数据类型：int, float, char
	假设我们要自己创建一种数据类型Vector，这种Vector定义了自身的数据内容和多种操作（如sort、find等）。而对Vector定义的统一规范，就是抽象数据类型（ADT）。
2. 向量ADT
	在C/C++语言中，向量是数组这一数据类型的推广和泛化。由一组元素按线性次序封装而成。
	+ 各元素与`[0, n)`内的秩一一对应
	+ 元素的类型不限于基本类型
	+ 操作、管理、维护更加简化统一与安全
	+ 可更为便捷地参与复杂数据结构的实现
3. 在Vector的copyFrom函数中，一开始申请两倍于要copy数组长度的空间，是为了保证在接下来足够长的时间内，不会因为需要扩容而打断copy的过程。
### 可扩充向量
---
1. 装填因子=实际所用容量/最大容量
2. 平均分析 vs. 分摊分析
+ 平均复杂度/期望复杂度
	根据数据结构各种操作出现概率的分布，将对应的成本加权平均。
+ 分摊复杂度
	对数据结构连续地实施足够多次操作，所需总体成本分摊至单次操作。
	注意，分摊复杂度是分摊到每个**操作**上的。例如教程中`C++ Vector`的加倍扩容操作，总体复杂度为O(n)，分摊时不是分摊到扩容，而是分摊到Vector操作上，所以分摊复杂度为O(1)。
### 无序向量
---
1. 在最好情况下和最坏情况下差距悬殊的算法，称为输入敏感(input-senstitive)算法。
	例如Vector的查找操作，最好情况下复杂度为O(1)，最坏情况下复杂度为O(n)。
### 有序向量：唯一化
---
1. 有序序列中，任何一对相邻元素都是顺序的。
### 有序向量：二分查找
---
1. 语义约定
+ 有序向量的查找方法（search）应该便于有序向量自身的维护，例如要向向量中插入元素时，需要返回合适的插入位置
+ 即使失败，也应该给出新元素适当的插入位置
+ 若允许重复元素，则每一组也需按照插入的次序进行排列
	前人给出了一个查找方法的约定：在有序向量区间`V[lo, hi)`中，确定**不大于e**的**最后一个**元素。
### 有序向量：Fibonacci查找
---
1. 思路：通过递归深度的不均衡，对转向成本的不均衡进行补偿。
2. 依赖顺序表分割查找算法中，如果算法总体模式不改变，那么对于任何的A[0, n), 总是选取A[λn]作为轴点，λ就是分割的比例。在`[0, 1)`内，λ如何取值最优的问题，就是设平均查找长度为α(λ)log2n，求α(λ)在λ为多少时最小？
	递推式如下：

	$$α(λ)log_{2}n=λ[1+α(λ)log_{2}(λn)]+(1-λ)[2+α(λ)log_{2}((1-λ)n)]$$
	
	递推的结果，当λ=0.6180339...（黄金分割比）时，得到的结果最小。也就是α(λ)为Fibonacci分割时复杂度常数项最小。
### 有序向量：插值查找
---
1. 假设已知有序向量中各元素随机分布的规律，则可以通过规律猜测并拟定要查找值位置，从而大幅提高查找效率。例如，各元素均匀独立分布在区间中，则查找节点大致位于节点值与区间值范围的比例处。
### 起泡排序
---
1. 起泡排序的改进
	+ 记录发生交换的状态，如果某一趟排序后向量没有发生交换，则这剩下的排序过程则没有必要再进行。假设剩下未迭代的向量元素为r个（实际上这r个元素已经是有序的），则算法的复杂度为$O(n^2-r^2)$。
	+ 在遍历并交换的过程中，记录最后一对逆序对的位置，并返回。在下一次遍历时，直接跳到该逆序对位置开始遍历，跳过的部分是有序的向量段。假设每次返回的位置为r1, r2, ... rk，则算法复杂度为$O(n^2-r_{1}^{2}-r_{2}^{2}-...-r_{k}^{2})$
2. 输入含重复元素时，算法的稳定性(stability)是更为细致的要求。稳定性指重复元素在输入、输出序列中的相对次序保持不变。以上起泡排序算法是稳定的。
### 归并排序
---
1. 归并排序的核心算法是二路归并。算法核心代码思路如下：
	```c++
	for(...; j < lb || k < lc; i++) {
		if(B[j] < C[k]) A[i++] = B[j++];
		if(B[j] >= C[k]) A[i++] = C[k++];
	}
	```
	由于每经过一次迭代，j和k钟至少有一个会加一。因此，merge总体迭代不超过 **O(n)** 次。累计只需要线性时间。
	而归并前的二分操作需要 **O(1)** 复杂度，分完以后总共有$log_2n$次归并操作，则从渐进意义上看归并排序的复杂度为 **O(nlogn)** 。

## 列表
---
### 接口与实现
---
1. 操作分为两类：
	(1) 静态：仅读取，数据结构的内容及组成一般不变：get, search
	(2) 动态：需写入，数据结构的局部或整体将改变：insert, remove
2. 数据元素的存储方式与组织方式也分为两种：
	(1) 静态：数据空间整体创建或销毁，数据元素的物理存储次序与其逻辑次序严格一致，支持高效的**静态**操作（如向量）。
	(2) 动态：为个数据元素动态地分配和回收的物理空间，逻辑上相邻的元素记录彼此的物理地址，在逻辑上形成一个整体，支持高效的**动态**操作（如列表）。
3. 列表(list)采用动态存储策略，其中的元素称为节点(node)。各节点通过指针或引用彼此连接，在逻辑上构成一个线性序列。相邻节点彼此互称前驱(predecessor)或后继(successor)，没有前驱/后继的唯一节点称为首(first/front)/末(last/rear)节点。
4. 构造列表前，先创建一个头节点(header)和一个尾节点(trailer)，作为哨兵。从逻辑上的秩表示，header, front, rear, trailer的秩分别为-1, 0, n-1, n。
### 无序列表
---
### 有序列表
---
1. 有序列表的查找复杂度为**O(n)** 。且不能通过像向量那样通过二分查找或Fibonacci查找进行优化。原因是列表是**循位置访问**的，如果采用二分查找，每次获得中点这一操作的复杂度不是O(1)而是**Θ(n)** ，则总体复杂度为**O(nlogn)** 。

### 选择排序
---
1. Bubblesort其实是一种选择排序(selectionsort)。
2. 使用选择排序，共迭代n次，在第k次迭代中
+ selectMax()操作复杂度为**Θ(n - k)**
+ remove()和insertBefore()的复杂度均为**O(1)**
	所以总体复杂度为**Θ(n^2^)** ，且不存在最好或最坏情况。
	尽管如此， 元素移动操作远远少于起泡排序。
	
### 插入排序
---
1. 在最好情况下，插入排序的复杂度为**O(n)** ，远好于选择的**Θ(n^2^)** 。
2. 在最坏情况下，插入排序的复杂度为**O(n^2^)** 。
3. 一组随机变量总和的期望，等于它们各自期望的总和，不论它们相关还是不相关。
4. 插入排序的平均性能与最坏情况是同阶的（ **O^2^** ）。因此虽然存在最好情况，但是最好情况发生的概率极低。
5. 记录逆序对时，将逆序对记录到**后者（较小者）** 中。这样计算总逆序对时，只需要统计所有逆序对较小者中数量总和，即可得到逆序对数量。记每个逆序对较小者中记录的逆序对数量为$i(p)$，则逆序对总和:
	$$I = \Sigma i(p)$$
	`I`就是插入排序所有定位过程中比较次数的累计总和，也就是该算法消耗时间的主要部分。而插入操作所花费时间的总和为O(1) * n = O(n)。因此花费的总时间为**O(I + n)**。

## 栈和队列
---
### 栈接口与实现
---
1. 基于向量或列表可以直接派生出栈。最好将向量首端作为栈底，将向量末端作为栈顶，这样所有的`push`, `top`, `pop`操作的复杂度均为**O(1)** 。

### 栈的应用——逆序输出(conversion)
---
1. 这种应用的特点是输出次序与处理过程颠倒；递归深度和输出长度不易预知。例如进制转换。


### 栈的应用——递归嵌套(stack permutation + parenthesis)
---
1. 具有自相似性的问题可递归表述，但分支位置和嵌套深度不固定。例如括号匹配。
	思路：消除一对紧邻的左右括号。
	实现方法：顺序扫描表达式，用栈记录已经扫描的部分。反复迭代，凡遇到`(`，则进栈；凡遇到`)`，则出栈。
	使用栈结构而不是简单的计数器进行括号匹配，优点是栈结构可以便捷地推广至多种括号并存的情况。例如`[ ( ] )`这种表达式应是失配的（不同括号范围交叉了），使用栈结构可以正确地发现这种失配。
	
### 栈的应用——栈混洗
---
1. 栈混洗总数：
	$$ SP(n) = catalan(n) =  \frac{(2n)!}{(n + 1)!n!} $$
2. 任意三个元素能否按某相对次序出现于混洗中，与其他元素无关。
	因此，对于任何$1<= i < j < k <= n$，`[...k, ...i, ...j, ....>` 必非栈混洗。
3. 括号匹配与栈混洗：每一栈混洗，都对应于栈S的n次push与n次pop操作构成的序列。
	因此，n个元素的栈混洗个数 **SP(n)** 也对应着n个括号的合法组合个数。
### 栈的应用——延迟缓冲(evaluation)
---
1. 特点：线性扫描算法模中， 在预读足够长之后，方能确定可处理的前缀。
### 栈的应用——中缀表达式
---
1. 中缀表达式使用两个栈的结构存储表达式，一个是操作数栈，一个是操作符栈。
2. 中缀表达式使用一个 **优先级表** 表示不同运算符的优先级。当当前运算符优先级比栈顶运算符优先级 **高** 时，当前运算符 **入栈** ；当当前运算符优先级比栈顶运算符优先级 **低** 时，先将栈顶运算符 **出栈** 进行计算，得到的结果压入操作数栈，不断循环直至当前运算符优先级比栈顶运算符优先级高，再将当前运算符 **入栈**。如果当前运算符与栈顶运算符优先级 **相同** （只可能出现在左右括号之间和表达式结尾符`$`），则将栈顶运算符弹出，并跳过当前运算符前往下一个符号（相当于消去了这两个运算符）。
	*注：实际中除`(`，`)`外优先级相同的运算符（如`+和-`，`*和\`等），在优先级表中，位于栈顶时优先级更高（也就是在表达式中先出现优先级更高），从而获得计算的优先权。*


### 栈的应用——逆波兰表达式（RPN）
---
1. 日常使用的表达式称为中缀式(infix)，RPN称为后缀式(postfix)。RPN不需要使用括号，即可表示带优先级的运算关系。作为补偿，需额外引入一个起分隔作用的元字符（比如空格）。在计算机中，RPN的逻辑更清晰，效率比中缀表达式效率更高。
2. 所有操作数在RPN中的先后顺序，与它们在中缀表达式中的次序是相同的。
3. 中缀式转化为逆波兰表达式：
	+ 原表达式：`(0!+1)^(2*3!+4-5)`
	+ 先将原中缀表达式所有运算符都使用括号显式地表示优先级
		```
		{ ( [ 0 ! ] + 1 ) ^ ( [ ( 2 * [ 3 ! ] ) + 4 ] - 5 ) }
		```
	+ 然后将运算符移到对应的右括号后
		```
		{ ( [ 0 ] ! 1 ) + ( [ ( 2 [ 3 ] ! ) * 4 ] + 5 ) - ) ^ } 
		```
	+ 整理可得逆波兰表达式
		```
		0 ! 1 + 2 3 ! * 4 + 5 - ^
		```
4. 所有操作数在RPN中的次序与它们在中缀表达式中的次序保持一致，操作符的顺序可能发生变化。 

## 二叉树
---
### 树的简介
---
1. 树这一数据结构可以认为是`List<List>`（一个二维的列表），它能结合Vector在静态操作上的优势和List在动态操作上的优势。
2. 任何一棵数的**边数**，等于**所有顶点度数之和**，等于**顶点个数减1**。令边的数量为e，顶点个数为n，则有：
	$$ e = \Sigma_{r \in V}degree(r) = n - 1 = \Theta(n) $$
	即任何一棵树的边数和顶点数是同阶的。

3. 度为k的树中第i层上至多有 **k^i-1^** 个结点。  
4. 深度为n的k叉树（即每个结点的度最大为k）上至多有$ \frac{k^n-1}{k - 1}$个结点。
5. 具有n个结点的k叉树的最小深度为$\lceil log_k(n(k-1)+1) \rceil$（即向上取整）。
3. 图V中的k+1个节点，通过E中的k条边一次相联，构成一条路径（path）。
4. 若节点之间均有路径，则称作连通图（connected graph）。若该图不含环路，则称作无环图（acyclic graph）。树是一种**连通无环图**，在保证连通条件下边最少，在保证无环条件下边最大。
5. 在树中，任一节点v与根之间存在**唯一路径**。
6. 不致歧义时，路径、节点和子树可相互指代。
7. v的深度：depth(v) = |path(v)|.
8. path(v)上的节点，均为v的**祖先（ancestor）**，v是它们的**后代（descendent）**。其中除了自身以外，称为真祖先/后代。
9. 树具有半线性的特征，在任一深度，v的**祖先**若存在，则**必然唯一**；若v的**后代**存在，则**不一定唯一**。由于图结构均不保证祖先唯一和后代唯一，因此是非线性结构。
10. 根节点是所有节点的公共祖先，深度为0。
11. **没有后代**的节点称作**叶子（leaf）**。
12. 叶子深度中的**最大者**，称为树的**高度**：height(v) = height(subtree(v))，因此**没有子树**的结点高度为**0**。
13. 特别地，空树的高度取作 **-1**。
14. depth(v) + height(v) <= height(Tree)。
### 树的表示
---
1. 采用“父亲”表示方法，向上访问和访问根结点只需要O(1)的时间复杂度，但向下访问时，不论是Child结点还是Sibling结点都需要O(n)的复杂度，因此不是一个好的表示树结构的方法。
2. 采用“孩子”表示方法，向下访问只需要O(d)的复杂度（d是节点的度），但向上访问父节点在最坏情况下需要遍历整个树，复杂度为O(n)。
3. 如果采用“父亲+孩子”的表示方法，可以结合以上两种方法的优势。但在构建孩子列表时，不同节点的孩子列表有可能相差悬殊，以至于出现访问某一个孩子需要O(n)的情况。因此需要更好的方式来存储。
3. 使用“长子-兄弟”的表示方法存储树的数据结构，对空间和时间上都有较好的优化。数据存储如下：

	|Rank|Value|First Child|Next Sibling|
	|---|---|---|---|
	||||

	每个节点所占用的空间都是常数，且彼此空间接近。
### 二叉树（binary tree）
---
1. 二叉树：节点度数**不超过2**的树。同一节点的孩子和子树，均以左、右区分。
2. 深度为k的节点，至多 **$2^k$** 个。
3. 含有n个节点、高度为h的二叉树中，**$h < n < 2^h+1$**。
4. 当n = h + 1时，退化为一条单链；**$n = 2^{h+1} - 1$**时，该二叉树为**满二叉树**。
5. **真二叉树**：所有节点的度数为0或2。
6. 所有的多叉树，通过 **“FirstChild-NextSibling”** 的表示方法，都可以转化为二叉树。其中左子节点为 **FirstChild** ，右子节点为 **NextSibling** 。
### 二叉树实现
---
1. 二叉树高度因类型不同而异，因此将 **`updateHeight()`** 方法设置为**虚函数（virtual）**，方便子类重写。
2. 在普通二叉树中，其高度的变化是由其左节点或右节点的高度发生变化引起的。在计算高度时，会取左节点和右节点两者的高度最大者。更新高度时，往往会引起父节点的高度变化且可能会层层递进直至根节点。此时，高度更新的时间复杂度线性正比与该节点的深度。即**O(n->depth)**。


### 先序遍历(Preroder Traversal)
---
1. 当某个函数的最后一步是调用另一个函数，则称这一调用为**尾调用**。当这一调用是调用自身时，称为**尾递归**。尾递归很容易改写成迭代形式，从而对函数进行优化。
2. 使用**栈**即可将递归形式的先序遍历改写成迭代格式。由于希望左子树先于右子树进行访问，因此入栈时**右孩子**先入栈，**左孩子**后入栈（FILO）。
3. 在先序遍历改进算法中，先沿**左侧链**不断向下访问，然后自左侧**最底部**的**右子树**依次向上访问（如下图）。因此在改进算法中，对左子树的根节点**直接进行访问**，然后将右孩子入栈，相对于改进前的算法，省去了左孩子入栈和出栈的操作。

![先序遍历改进思路](http://onku98v3l.bkt.clouddn.com/%E5%85%88%E5%BA%8F%E9%81%8D%E5%8E%86%E6%94%B9%E8%BF%9B%E6%80%9D%E8%B7%AF.jpg)

![先序便利改进思路模型](http://onku98v3l.bkt.clouddn.com/%E5%85%88%E5%BA%8F%E9%81%8D%E5%8E%86%E6%94%B9%E8%BF%9B%E6%80%9D%E8%B7%AF2.png)

### 中序遍历(Inorder Traversal)
---
1. 中序遍历的迭代算法中，需要将**左结点**不断入栈，直至**最左侧结点**，访问它，然后访问掉**右子树**。访问结束后弹出栈中的下一个结点并访问他，再访问右子树。逐层迭代，直至全部结点访问完毕。具体思路如下图。

![中序遍历迭代版思路](http://onku98v3l.bkt.clouddn.com/%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86%E6%80%9D%E8%B7%AF.png)


![中序遍历迭代版思路2](http://onku98v3l.bkt.clouddn.com/%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86%E8%BF%AD%E4%BB%A3%E7%89%88%E6%80%9D%E8%B7%AF2.png)

2. 中序遍历迭代版本的时间复杂度仍然是**O(n)**，而且常系数上远胜于递归版本。

### 后序遍历
---
1. 后序遍历的迭代方法需要先找到**第一个被访问**的节点，将其**祖先**及其**右兄弟**（如果存在）用**栈**保存。这样原问题就被分解为依次对若干棵右子树的遍历问题。
	其正确性恪归纳证明，每个节点出栈后，以之为根的子树已经完全遍历。而且其右兄弟r（如果存在）就在栈顶。

![后序遍历迭代思路](http://onku98v3l.bkt.clouddn.com/%E5%90%8E%E7%BB%AD%E9%81%8D%E5%8E%86%E6%80%9D%E8%B7%AF.png)

2. 后序遍历的方法适合于表现RPN（逆波兰表达式）。

### 层次遍历
---
1. 在先序/中序/后序遍历中，都存在底层次比高层次结点先访问的可能（即逆序遍历）。而**层次遍历**则严格按照顺序进行遍历。这一遍历方法可以通过**队列**实现。注意，这里入队时的顺序是**左先右后**。实现代码和思路如下图：

![层次遍历实现](http://onku98v3l.bkt.clouddn.com/%E5%B1%82%E6%AC%A1%E9%81%8D%E5%8E%86%E5%AE%9E%E7%8E%B0.png)

### 二叉树的重构
---
1. 只需要中序序列，加上先序和后序序列之一（**中序 + [先序 | 后序]**），即可还原二叉树的拓扑结构。只通过先序序列和后续序列，是不能重构二叉树的。
2. 在二叉树为**真二叉树**的情况下，可以通过先序序列和后序序列重构二叉树。

## 图
---
### 图的概述
---
1. 图的数学描述：G = (V; E)
	一般用n表示点集，n  = |V|；用e表示边集，e = |E|。
2. **邻接关系（adjacency）** 描述的是顶点与顶点之间的关系；**关联关系（incidence）** 描述的是顶点与边之间的关系。
3. 若邻接顶点u和v的次序无所谓，则(u, v)为无向边。所有边均为无方向的图，为**无向图（undigraph）** 。
4. 反之，**有向图（digraph）**中均为有向边，u，v分别称作边(u, v)的**尾（tail）**、**头（head）**。
5. 两种边均有的图，称为混合图（mixed graph）。
6. 有向图可以描述无向图和混合图，因此主要讨论有向图。
7. 如果一条路径中不含重复结点，则称为**简单路径（simple path）**。
8. 一条包含k个节点的路径中，如果$v_0 = v_k$，则称为**环路(cycle)** 。
9. 如果在一个有向图中不包含环路，则称为**有向无环图（Directed Acyclic Graph, DAG）**。
10. 欧拉环路：恰好经过所有的**边**一次；哈米尔顿环路：恰好经过所有的**顶点**一次。
11. 图G = (V; E)的子图T = (V; F)若是树，则为其支撑树（spanning tree）。一个图的支撑树通常不唯一。
12. 各边e均有对应的权值wt(e)，则为带权网络。
13. 统一网络的支撑树中，总权重最小者为最小支撑树（minimun spanning tree, MST）。

### 邻接矩阵
---
1. 邻接矩阵（adjacency matrix）：用二维矩阵记录顶点之间的邻接关系。A(i, j) = 1或0，表示顶点i与j之间存在或不存在一条边。空间复杂度为**Θ(n^2^)** ，与图中实际的边数无关。有向图中，**行为尾（tail）**，**列为头（head）**。适用于**稠密图**。
2. 关联矩阵（incidence maxtrix）：顶点个数为行（n行），边个数为列（e列）的矩阵。空间复杂度为**Θ(n^3^)** ，空间利用率小于2/n，每一列中最多只有两个元素为1。
3. 平面图（planar graph）：可嵌入平面的图（不相邻的两条边不相交）。
4. 欧拉公式：对于任何平面图，有**v - e + f - c = 1**。其中v为顶点数，e为边数，f为区域片个数，c为连通域个数。
5. 对于平面图，边的总数不会超过顶点的总数，空间利用率约等于1/n。
6. 邻接表的空间复杂度：
	+ 有向图：O(n + e)。
	+ 无向图：O(n + 2 * e) = O(n + e)。
	因此邻接表适用于稀疏图，例如平面图。

### 广度优先搜索
---
1. 如果一个图中存在多个连通域（也就是多个相互不连通的子图），使用BFS搜索时可以逐一检查所有顶点，一旦遇到尚未发现的顶点（UNDISCOVERED），即从该顶点出发启动一次BFS。易证明，BFS搜索在一个连通域内仅进行一次遍历搜索，因此复杂度不过是连通域的个数 × 遍历复杂度。
2. BFS算法过程：

	```mermaid!
	graph TD;
		访问顶点S-->依次访问s所有尚未访问的邻接顶点;
		依次访问s所有尚未访问的邻接顶点-->依次访问它们尚未访问的邻接顶点;
		依次访问它们尚未访问的邻接顶点-->直至没有尚未访问的邻接顶点;
	```
	
3. 图的广度优先遍历，可以认为是树的层次遍历的一种特例。
4. 图的遍历算法的核心思想，是将图转化成支撑树这一半线性结构，再由树的遍历方法转化成线性结构，从而实现遍历。
5. 节点的status如果被标记为TREE，则将构成遍历支撑树；如果被标记为CROSS，则在构造遍历支撑树时会丢弃。
6. BFS算法从渐进意义上复杂度为O(n^2^ + e)。但在实际中，`for`循环语句中的基本操作十分简单，而且组成每一个行向量在物理上是**连续、紧凑排列**的，有利于**高速缓存**机制发挥作用，访问速度上会提高5-6个数量级。如果使用邻接表，`for`循环带来的O(n)复杂度几乎可以忽略掉，而使BFS算法的复杂度达到**O(n + e)** 。
7. 广度优先搜索表示最短路径
	BFS过程中，队列Q犹如一条贪吃蛇，以s节点为遍历起点，有
	+ 顶点按dist(s)单调排列
	+ 相邻顶点的dist(s)相差不超过1
	+ 首、末顶点的dist(s)相差不超过1
	+ 由树边（TREE）联接的顶点，dist(s)恰好相差1
	+ 由跨边（CROSS）联接的顶点，dist()最多相差1
8. BFS树中从s到v的路径，就是原图中二者的最短通路。


### 深度优先搜索
---
1. 在深度优先搜索（DFS）中，如果发现了一条BACKWARD边，则表示至少出现了一个环路（loop）。
2. 对于DFS，在无向图中，后向边与前向边不予区分，跨边没有。
3. **dtime**是**被访问到的时间**，**ftime**是**访问完成（被标记为VISITED）的时间**。
4. DFS一次遍历完成后，所有访问完成（即标记为VISITED）的节点构成一个**可达区域**。
5. 括号引理（嵌套引理）
	+ 顶点的活动期：active[u] = (dTime[u], fTime[u])
	+ 括号引理（Parenthesis Lemma）：给定有向图G = (V, E) 及其任一DFS森林，则有下图关系。

	![嵌套引理](http://onku98v3l.bkt.clouddn.com/%E5%B5%8C%E5%A5%97%E5%BC%95%E7%90%86.png)

	根据嵌套原理，可以在**O(1)** 的时间内，判断两个节点在遍历树中是否存在直系血缘关系。
	
6. 深度优先搜索算法的时间复杂度为**O(n + e)** 。 

### 拓扑排序
---
1. 在**有向无环图**中，拓扑排序必然存在。而非有向无环图则不一定存在拓扑排序。
2. 所谓的拓扑排序，就是构造一个与指定偏序集相容的全序集。
3. 算法A：顺序输入零入度顶点

	![拓扑排序——顺序输出零入度顶点](http://onku98v3l.bkt.clouddn.com/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E2%80%94%E2%80%94%E9%A1%BA%E5%BA%8F%E8%BE%93%E5%87%BA%E9%9B%B6%E5%85%A5%E5%BA%A6%E9%A1%B6%E7%82%B9.PNG)
	
	注意算法中，入栈的条件与顶点访问完成后的操作。入栈的条件是顶点v的邻接顶点u若**入度仅为1**，则u入栈；顶点v访问完成后将会删除顶点v及其关联边（使顶点v的邻接顶点入度减1）。总体复杂度为O(n + e)。
	
4. 算法B：逆序输出零出度顶点

	![拓扑排序——逆序输出零出度顶点](http://onku98v3l.bkt.clouddn.com/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E2%80%94%E2%80%94%E9%80%86%E5%BA%8F%E8%BE%93%E5%87%BA%E9%9B%B6%E5%87%BA%E5%BA%A6%E9%A1%B6%E7%82%B9.PNG)
	
	该算法基于DFS。复杂度与DFS相当，也是O(n + e)。

### 优先级搜索
---
1. 以上遍历算法的区别，仅仅在于选取顶点进行访问的次序。
	+ 广度：优先访问与更早被发现的顶点相邻接者
	+ 深度：优先访问与更晚被发现的顶点相邻接者
	每一种选取策略都等效于，给所有顶点赋予不同的优先级，而且随着算法的推进不断调整，而每一步迭代所选取的顶点，都是当时的优先级最高者。因此包括BFS和DFS在内的几乎所有图搜索，都可以纳入统一的框架，称为优先级搜索（PFS）。
2. 优先级搜索中，在算法的初始化阶段，通常先将顶点的优先值统一置为最大————也就是优先级最低。

### 最小支撑树
---
1. 若图G为一带权网络，则每一棵支撑树的成本即为其所采用各边权重的总和。在G的所有支撑树中，成本最低者称为最小支撑树（minimum spanning tree, MST）。
2. 图G = (V; E)中，顶点集V的任一非平凡自己U及其补集V\U都构成G的一个割(cut)，记作(U : V\U)。若边uv满足u∈U且v不属于U，则称作该割的一条跨越边（crossing edge）。

#### Prim算法
---
3. Prim算法的正确性基于以下事实：最小支撑树总会采用联接每一割的最短跨越边。
4. Prim算法的简单描述：
	+ 输入：一个加权连通图，其中顶点集合为V，边集合为E。
	+ 初始化：$V_{new} = \{x\}$，其中x为集合V中的**任一顶点**（起始点）。$E_{new} = \{\}$，为空。
	+ 重复下列操作，直至$V_{new} = V$：
		+ 在集合E中选取权值最小的边$ < u, v > $，其中u为集合$V_{new}$中的元素，而v不在$V_{new}$集合中，并且v∈V（如果存在有多条满足前述条件即具有相同权值的边，则可任意选取其中之一）。
		+ 将v加入集合$V_{new}$中，将$ < u, v > $边加入集合$E_{new}$中。
	+ 输出：使用集合$V_{new}$和$E_{new}$来描述得到的最小生成树。
5. Prim算法的复杂度：设顶点数为v，边数为e。
	+ 邻接矩阵：$O(v^2)$
	+ 邻接表：$O(elog_2v)$
6. Prim算法适合**顶点较少，边较多**的情况。

#### Kruskal算法
---
1. Kruskal算法框架：
	+ 记Graph中有v个顶点，e个边
	+ 新建图$Graph_{new}$，$Graph_{new}$中拥有原图中相同的e个顶点，但没有边
	+ 将原图Graph中所有e个边按权值从小到大排序
	+ 循环：从权值最小的边开始遍历每条边
		if 这条边连接的两个节点于图$Graph_{new}$中不在同一个连通分量中
			添加这条边到图$Graph_{new}$中
		直到图Graph中所有的节点都在同一个连通分量中
	+ 整个过程共迭代n-1次，选出n-1条边
2. 时间复杂度：
	+ 邻接表：$O(eloge)$
3. Kruskal算法适合**顶点较多，边较少**的情况。

### 最短路径
---
1. 连通图中，s到每个顶点都有至少一条最短路径。
2. 就同一起点s而言，任何最短路径的前缀，也是一条最短路径。
3. 就统一起点s而言，所有最短路径并不含回路，因此必构成一棵树。
4. SPT（shorttest path tree） ≠ MST（minimum spanning tree），前者是每个顶点到起点的权值最小， 后者是各边权重的总和最小。

#### Dijkstra算法
---
1. Dijkstra算法过程：
	+ 初始时，S只包含源点，即S＝{v}，v的距离为0。U包含除v外的其他顶点，即:U={其余顶点}，若v与U中顶点u有边，则$ < u,v > $正常有权值，若u不是v的出边邻接点，则!$ < u,v > $权值为∞。
	+ 从U中选取一个距离v最小的顶点k，把k，加入S中（该选定的距离就是v到k的最短路径长度）。
	+ 以k为新考虑的中间点，修改U中各顶点的距离；若从源点v到顶点u的距离（经过顶点k）比原来距离（不经过顶点k）短，则修改顶点u的距离值，修改后的距离值的顶点k的距离加上边上的权。
	+ 重复步骤b和c直到所有顶点都包含在S中。
2. 不采用最小优先队列的Dijkstra算法时间复杂度是$O(|V|^2)$，V为顶点个数。通过斐波那契堆实现的Dijkstra算法时间复杂度是$O(|E| + |V|log|V|)$，其中E是边数。
3. Dijkstra算法**不允许**出现负权边，自然也不允许出现负权回路。

#### Floyd-Warshall算法
---
6. Floyd-Warshall算法：给定图G，计算其中所有点对之间的最短距离，也就是**多源最短路径**问题。
7. FW算法的核心思想：动态规划。
8. FW算法的详细描述：见[只有五行的Floyd最短路算法](http://developer.51cto.com/art/201403/433874.htm)。
9. FW算法的时间复杂度为**$O(n^3)$** ，与n次调用Dijkstra的时间复杂度相同，但形式简单、算法紧凑、便于实现，允许负权边（不能有负权环路）。空间复杂度需要维护一张`n*n`的表格，为**$O(n^2)$** 。
10. FW算法边权可以为负数，但仍然不允许负权回路。
